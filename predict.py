#Prediction code - modified with shift in datapoints

import os
import sqlite3
import pandas as pd
from sklearn.preprocessing import StandardScaler
import joblib
import numpy as np
import requests
import xgboost as xgb

# Paths and configurations
DATA_DB = 'joined_data.db'
PREDICTIONS_DB = 'data/predictions_v1.db'
MODELS_DIR = 'models_v1'
DATA_FOLDER = 'data_v1'

# Ensure folders exist, including the data folder for the predictions database
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(DATA_FOLDER, exist_ok=True)
os.makedirs(os.path.dirname(PREDICTIONS_DB), exist_ok=True)  # Ensure data folder exists

def download_database():
    url = 'https://raw.githubusercontent.com/chiragpalan/final_project/main/database/joined_data.db'
    response = requests.get(url)
    if response.status_code == 200:
        with open(DATA_DB, 'wb') as f:
            f.write(response.content)
        print("Database downloaded successfully.")
    else:
        raise Exception("Failed to download the database.")

def get_table_names(db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = [row[0] for row in cursor.fetchall()]
    conn.close()
    return tables

def load_data_from_table(db_path, table_name):
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
    conn.close()
    return df

def clean_data(X, y, dates):
    """Remove rows with NaN or infinite values from a NumPy array and keep data aligned."""
    mask = np.isfinite(X).all(axis=1)  # Identify rows without NaN or inf
    X_clean = X[mask]
    y_clean = y.reset_index(drop=True)
    dates_clean = dates[mask].reset_index(drop=True)
    return X_clean, y_clean, dates_clean

def extract_percentiles(predictions):
    """Calculate 5th and 95th percentiles from an array of predictions."""
    p5 = np.percentile(predictions, 5, axis=0)
    p95 = np.percentile(predictions, 95, axis=0)
    return p5, p95

def random_forest_predictions(model, X_scaled):
    all_preds = [est.predict(X_scaled) for est in model.estimators_ if hasattr(est, 'predict')]
    p5, p95 = extract_percentiles(all_preds)
    main_prediction = model.predict(X_scaled)
    return main_prediction, p5, p95

def xgboost_predictions(model, X_scaled):
    # Convert X to DMatrix format for XGBoost compatibility
    dmatrix = xgb.DMatrix(X_scaled)

    # Get contributions from each tree for each sample
    contributions = model.get_booster().predict(dmatrix, pred_contribs=True)

    # Remove the last column as it's the bias term; each row now has 800 columns (one per tree)
    individual_tree_contributions  = contributions[:, :-1]

    # Compute predictions from each individual tree (no cumulative effect)
    individual_predictions = np.cumsum(individual_tree_contributions, axis=1)

    p5 = np.percentile(individual_predictions, 5, axis=1)
    p95 = np.percentile(individual_predictions, 95, axis=1)

    main_prediction = model.predict(X_scaled)
    return main_prediction, p5, p95

def gradient_boosting_predictions(model, X_scaled):
    n_estimators = model.n_estimators
    individual_predictions = np.zeros((X_scaled.shape[0], n_estimators))

    # Collect predictions from each estimator
    for i in range(n_estimators):
        individual_predictions[:, i] = model.estimators_[i, 0].predict(X_scaled)

    # Calculate percentiles
    p5 = np.percentile(individual_predictions, 5, axis=1)
    p95 = np.percentile(individual_predictions, 95, axis=1)
    main_prediction = model.predict(X_scaled)

    return main_prediction, p5, p95

def save_predictions_to_db(predictions_df, prediction_table_name):
    try:
        conn = sqlite3.connect(PREDICTIONS_DB)
        predictions_df.to_sql(prediction_table_name, conn, if_exists='replace', index=False)
        conn.close()
        print(f"Predictions successfully saved to table: {prediction_table_name}")
    except Exception as e:
        print(f"Error while saving to database: {e}")

def process_table(table):
    df = load_data_from_table(DATA_DB, table)  # Drop rows with NaN in the DataFrame
    df = df.dropna(subset=[col for col in df.columns if col != 'target_n7d'])
    print(df.columns)
    if 'Date' not in df.columns:
        raise KeyError("The 'Date' column is missing from the data.")

    X = df.drop(columns=['Date', 'target_n7d'], errors='ignore')
    y_actual = df['target_n7d']
    dates = df['Date']

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Remove rows with NaN or inf in X_scaled, and align y_actual and dates
    X_scaled, y_actual, dates = clean_data(X_scaled, y_actual, dates)

    model_types = ['random_forest', 'gradient_boosting', 'xgboost']
    prediction_functions = {
        'random_forest': random_forest_predictions,
        'gradient_boosting': gradient_boosting_predictions,
        'xgboost': xgboost_predictions
    }

    # Create an empty DataFrame to store combined predictions
    combined_predictions = pd.DataFrame({'Date': dates, 'Actual': y_actual})

    for model_type in model_types:
        model_path = os.path.join(MODELS_DIR, f"{table}_{model_type}.joblib")
        if not os.path.exists(model_path):
            print(f"Model file not found: {model_path}")
            continue

        model = joblib.load(model_path)

        # Predict using the appropriate function
        prediction_func = prediction_functions[model_type]
        try:
            main_prediction, p5, p95 = prediction_func(model, X_scaled)

            # Add predictions to the combined DataFrame
            combined_predictions[f'Predicted_{model_type}'] = main_prediction
            combined_predictions[f'5th_Percentile_{model_type}'] = p5
            combined_predictions[f'95th_Percentile_{model_type}'] = p95

        except Exception as e:
            print(f"Error predicting with {model_type}: {e}")

    # Save the combined predictions DataFrame to the database
    prediction_table_name = f"prediction_{table}"
    save_predictions_to_db(combined_predictions, prediction_table_name)

def sort_and_shift_tables():
    # Connect to predictions_v1.db
    conn = sqlite3.connect(PREDICTIONS_DB)
    cursor = conn.cursor()
    
    # Get list of tables
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = [row[0] for row in cursor.fetchall()]

    # For each table, sort by 'Date' and shift rows up by 7 units
    for table in tables:
        print(f"Processing table: {table}")
        df = pd.read_sql_query(f"SELECT * FROM {table}", conn)

        # Ensure the 'Date' column exists
        if 'Date' not in df.columns:
            print(f"Skipping table {table} because 'Date' column is missing.")
            continue

        # Sort by Date
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values(by='Date', ascending = False)

        # Shift rows up by 7 units, keeping the 'Date' column as is
        df_shifted = df.shift(-7)
        df_shifted['Date'] = df['Date']  # Keep Date column unchanged

        # Save the updated table back into the database
        df_shifted.to_sql(table, conn, if_exists='replace', index=False)

    conn.close()
    print("Tables sorted and rows shifted successfully.")

def main():
    download_database()
    tables = get_table_names(DATA_DB)

    for table in tables:
        try:
            process_table(table)
        except Exception as e:
            print(f"Error processing table {table}: {e}")

    # After saving predictions, sort and shift tables in predictions_v1.db
    sort_and_shift_tables()

    if os.path.exists(PREDICTIONS_DB):
        print(f"Predictions database created at: {PREDICTIONS_DB}")
    else:
        print("Predictions database was not created.")

if __name__ == "__main__":
    main()
